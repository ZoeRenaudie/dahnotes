---
title: Les phénomènes sans échelle à l'origine de la révolution numérique
description: conférence de Jean-Philippe Magué, CRIHN
author: ouvroir
date: 2022-12-01
draft: true
tags:
    - cr
    

---
# Les phénomènes sans échelle à l'origine de la révolution numérique

Des études initiales en informatique, mais doctorat en sciences cognitive (linguistique et modélisation orientée informatique). Les années 2000, moment où les DH prennent de l’ampleur et commencent à s’institutionnalisé, donc naturellement intégré cette communauté.

Toujours envisagé la langue comme un système dynamique à l’intérieur de la société, et un pied dans la communauté des systèmes complexes. À travers ces différents ancrages disciplinaires, eu l’occasion de réfléchir à ce que signifie vivre dans une société numérique. Qu’est-ce qu’il fait que dans une société numérique il y a des facteurs pour la transition.

Souvent évoque que le numérique a un caractère révolutionnaire du fait qu’il permet de profuire des volumes importants de données. On explique ainsi que depuis le de l’ère numérique la quantité de données explose et en croissance exponantielle. Mais ces discours manque le point important concernant cette explosion de données.
En réalité du point de vue historique, rien de nouveau du point de vue de l’augmentation exponantielle de la proportion de données : exemple le livre au XIXe, le manuscrit en Europe du 6e au 19e siècle. Il n’est donc pas dit que cette production exponantielle de données soit spécifique au XXIe.

Depuis qu’il est possible de rendre compte du savoir du passé. Développement exponantielle et technologies d’inscription de l’information ont soutenu cette explosion.

Explosion de données pas une conséquences de nos technologies. Mais plutôt parce que nous sommes une espère qui produit de l’information que l’on a besoin de ce genre de technologie pour traiter cette croissance : exemple invention du web, invention de l’informatique, production de ms qui a explosé à la fin du 11e siècle après la reprise de Tolède où les Espagnols redécouvre un grand nombre de ms grecs et latins.

Infobésité 12e et 13e qui conduit à des technologies de traitement de l’information : catalogage de bibliothèque, système de rubricage pour proposer des outillages du texte pour proposer des entrées non-linéaires dans le texte. (cf. Ann Blair)

Technologies conséquence de notre tendance à produire des données, mais aussi elles qui vont soutenir cette croissance exponentielle. Il y a donc une certaine circularité dans cette croissance.

Si ce n’est pas dans la production de données que ces technologies sont révolutionnaies alors où ?

Il y a une sorte de rétroaction qui se fait entre les outils que l’on met en place pour observer les structures sociales et les représentations que l’on créée qui influencent les structures sociales.

Mise en place 18e 19e siècle. Et cette boucle de rétroaction mise en place. Ce que les technologies numériques font, et sont en train de faire, c’est conduire à un nouvel ensemble de représentations qui vont être incorporées par les gens qui font la société. Pas changement brusque mais substitution progressive.

Outils statistiques se phénomènes sur des phénomènes avec échelle alors que les technologies numériques se focalisent sur des phénomènes sans échelle.

Combien mesure-t-on ? En réalité tous plus ou moins la même taille, répartition autour d’une taille moyenne selon une courbe gaussienne. Dès lors possible d’imaginer une personne typique.

Si au lieu de mesurer les gens, on mesure les villes. Combien d’habitants. Taille des villes en Fr par nombre d’habitant. Plus de 70% font moins de 1 000h. La majorité (70%). On ne dirait pourtant pas qu’il s’agit des villes typiques. On ne représente donc jamais les choses de cette manière, mais sous forme de graphiques logarithmiques qui permettent de voir que tout n’est pas à zéro et qu’il existe donc des villes de toutes les tailles. Paris est un million de fois plus grand que les plus petites villes. Les plus grandes mégalopoles, 100M de fois plus grand...

Un rapport 100M homme aussi grand que la lune !
Des grands nombres qui sont difficiles à envisager.
100Milliards de fois, distinction entre les plus pauvres et les plus riches. Si la personne la plus pauvre, 1m70 Musk grand comme la distance de la terre au soleil.

Des phénomènes avec échelle et Des phénomènes sans échelles qui n’ont pas de répartition.

Comment le sait-on ?
- on échantillonne, distribution qui correspond à distribution sous-jascente
- pour la taille des villes, essaye faire la même chose mais là ne fonctionne pas  vraiment.

L’échantillon est mauvais, il ne contient que très peu d’information à propos de la distribution sous-jacente. On rate la longue traine.
Cette technique d’échantionnage est mauvaise car la distribution n’est pas égale.

Il existe donc vraiment deux types d’événements différents.
Des phénomènes avec échelle : où les événements rares et fréquents ne sont pas très différents. Les événéemnets rapresne sont pas très importante te sont très rares. 

Phénomène avec échelle. Les événements rares et fréquents sont rtès différentes. les événemenst rares sont très importantes (et ne sont pas si rares)
Besoin pour les phénomènes sans échelle, d’avoir un échantillon très grand ou de regarder l’ensemble de la population.

Pour les traiter, on a besoin de moyens technologiques différents.

## Dév stats

Découverte de la loi normale au 19e siècle et d’un arsenal statistique qui est au cœure des interactions entre représentations et sociétés au 19es s.

Combinaisons d’informations Mayer (astronome)

Théorisation de la loi des erreurs, Carl Friedrich Gauss 1809, Pierre-Simon de Laplace 1810

Voir comment une fois cet outil statistique mis en place, il est passé au niveau des sciences sociales.

L’homme Moyen Adolphe Quetelet, 1835
Un astronome (directeur obs royal de Belgique) qui s’est pour plusieurs raisons mis à étudier les populations avec les outils mathématiques de l’astronomie en affirmant que la loi normale pouvait s’appliquer. Notamment qu’en répétant les mesures, on réduit les erreurs.

Dans ce système chaque personnes une réalisation de ces erreurs autour d’une variation qui est la valeur attendue. Cette moyenne, est une personne idéale. Un idéal type. Weber !

Erreur décrite par la distribution vers une variation.
Quetelet ne fait pas ça à un moment quelquonque. Première moitié du 19e s. moment où les états commencent à créer diverses institutions pour mesurer la société.

"During the years 1820-1840 the rate of increase in the printing of numbers appears to be exponentioal [...]" Ian Hacking, 1983. Biopower and the avalanche...

Pour quantifier la société, on est alors obligé de mettre les gens dans des cases. Pour ce faire il faut bien que les objets ou les concepts tombent dans une catégorie ou une autre. Cette quantification a généré une avalanche de catégories pour compter les gens.

Cf. Desrosières. Activité de créer des catégories pour compter les choses, pas uniquement le reflet du monde mais transforme autrement (2013 sociologie...)

Ex. La ménagère de moins de 50 ans qui une fois qu’elle existe va donner lieu à des représentations. 

Autre exemple de Francis Galton.
Cousin de Charles Darwin. Inventeur de la régression, inventeur de la corrélation. Fondateur de l’eugénisme. Avec son élève Pearson, mise en place superposition de photographies pour trouver le visage moyen du tuberculeux.

Ici idéal vu à droite de la distribution. Eugénisme. Là où Quetelet voyait l’idéal à la moyenne, ici l’idéal à droite. Eugénisme développé par nombre de régimes politiques nocifs. Autre forme de rétroaction des objets statistiques sur la société.

Voir comment loi normale idéalisée et devenu le pilier de la compréhension du monde social.

Pourtant dès la fin du 19e siècle, se passe des choses intéressantes.
Vilfredo Pareto s’est intéressé à la distribution des revenus.
1896, "Certains auteurs, en se laissant guider par des conceptions théoriques, donnent à la partie inférieure de la courbe la forme stv, fig. 50. La statistique ne nous fournit aucune indication en ce sens. Il est donc fort probable que la partie stv est très écrasée, et que la courbe réelle affecte une forme analogue à celle qu’indique la fig. 51."

Ce que réalise Pareto, c’est que les revenus n’ont pas d’échelle.
Tombe sur une courbe où certaines valeurs deviennent extrême. 

Quelque chose resté longtemps confidentiel. 
Yule, 1925 sur la distribution des espaces (18770 espèces, 2203 genres dans 4 familles)
Zipf, 1935 fréquence des mots (Ulysse, de James Joyce)
Benford 1938, distribution du chiffre de gauche dans des "données naturelles". Repéré en 1881 par Newcomb par l’usure des tables de logarithme
20222 observations 335 rivièves, 3259 villes

Pour observer ces phénomènes ne peut se contenter de petits échantillons. Tant qu’est dans un monde analogique difficile. Plus simple avec le numérique. Envisager les technologies numériques comme des instruments qui nous permettent de voir des phénomènes sans échelle.

Le téléscope voir des phénomènes très loin. Le microscope voir des choses très petites. Les technologies numériques choses sans échelles.

Hors de nos capacités perceptuelles. Pourquoi prend-on le logarithme ?
Pas assez de pixels, à 100 pixels par cm, il faudrait un écran de 10m sur 250m.
L’œil pas capable de voir simultanément des objets de taille de différente au-delà de 3 ou 4 ordres de grandeur.

or, taille des villes 7 ordres de grandeur
distribution des richesses, 11 ordres de grandeurs
gamme dynamique est de l’œil de l’ordre 1000

Mais nos capacités cognitives ne nous permet pas de tout nous représenter. Exemple fourmi et espèce, genre, tribu, sous-famille, famille, super-famille, infra-ordre, sous-ordre, ordre, infra-classe, sous-classe, classe, sous-embranchement, embranchement, règne.

Quand range les objets dans des catégories mentales, un niveau de base où s’effectue la perception et qui va nous permettre d’apréhender le monde.
Notre système de catégorisation s’articule autour d’un niveau de base, rosch et al, 1978

C’est le niveau où
- s’effectue la perception
- ...

Ces niveaux exhibent un effet de prototype. Quand on regarde des dessins de fourmis, certains exemplaires sont plus centraux, typiques. Certains exemplaires sont plus périphériques.

ex. Penser à un outil et une couleur. => marteau rouge

Notion de catégorie qui est très proche de la loi normale et de la manière dont l’idéal type est constitué. Nous sommes en quelque sorte cablés pour voir des phénomènes avec échelle. Notre système de représentation est calqué pour voir des phénomènes avec échelle. Normal car nous sommes adpatés pour appréhender le monde à une échelle donnée.

Nous ne percevons / conceptualisons pas les phénomènes sans échelle spontanément.
Pour ce faire, tendance à les découper en catégories plus fines pour les appréhender. Effort d’abstraction nécessaire pour décomposer en des catégories des éléments qui font partie d’un continuum complet.

- hameau, village, ville
- ruisseau, riviève, fleuve
- crique, baie, golfe

On construit des systèmes de catégories liés à notre expérience et difficile de voir qu’un continuum complet.

Nous sommes plus ou moins aveugles à ces phénomènes sans échelle dont la prise de conscience récente. 20e s. Et explosion de leur découverte avec l’explosion des capacités de calculs qui nous permettent de les voir.

Nous permet de voir qu’il y a partout de ces phénomènes et que le monde qui nous paraissait construit autour de catégories statistiques selon la répartition normale, seulement une partie du monde.

[The Long Tail](https://en.wikipedia.org/wiki/The_Long_Tail_(book)), Chris Anderson qui affirme que les buisness model des sociétés internets sont basés sur les phénomènes sans échelle. Exemple plateforme musicale Rhapsodie 2005, nb téléchargements titre plus populaires, mais aussi longue traine. Titres les plus rares très peu téléchargés mais sont proposés grace au numérique, alors que dans un magasin physique en raison de la place limité ne peut proposer que les titres les plus vendeurs.

Les services de streaming fonctionnent tous de la même façon. Et buisiness modèle repose sur le fait que même si ces morceaux peu téléchargés, autant d’écoute que la masse des premiers titres plus téléchargés mais moins nombreux.

Nos goûts musicaux ne sont pas des phénomènes avec échelle ? [pas évident, plutôt notre consommation].

Le modèle économique de Google, idem. Fondé sur la publicité
Publicité des années 2000, peu de lieu d’affichage, vu par beaucoup de monde. On présente les produits susceptibels d’intéresser les "visionneurs". En raisonnant sur des catégories du type "jeune de moins de 25 ans", ménagère de plus de 50 ans.
Modèle économique de Google qui se propose d’individualiser les publicité. Change de modèle car énormément de lieu d’affichage avec possibilité d’affichage individuel.

Ne va plus essayer de cibler le plus grand nombre, cette fois-ci essaye de tirer parti de l’ensemble des préférences de chacun avec des publicités qui n’intéressent pas nécessairement grand monde mais qui permettent de faire de l’argent sur la longue traine.

En quoi les technologies numériques révolutionnaires ? Car opèrent cette bascule entre des outils statistiques pour représenter les populations qui s’intéresse nt aux phénomènes avec échelles, et des représentation sans échelle. Possibilité de mettre en évidence ces phénomènes sans échelles, construire des buisness modèle fondés sur eux, et nous permettent de nous rendre compte que les catégories construites pas si rigides. Passe alors à un monde où ces catégories n’ont pas toujours de raison d’exister (autres façons d’exister que simple variance à la norme). Conduit à un affaiblissement de nos catégories.

Plateformes numériques comme YouTube, Wikipedia, Uber permettent toute de produire un éffondrement de nos catégories. YouTube versus télé et choix de qui passe à la TV. Plus claire distinction entre producteur et récepteur. Pour autant effacement qui se construit dans une idée de phénomènes sans échelle. Pas parce que tous devenus producteurs de contenus qu’aura un producteur de contenu type. Au contraire, va se répartir le long d’un spectre avec un ordre de grandeur très grand.

Quelques exemples, sur l’effacement des catégories.
Courbe fréquence de recherche dans Google Claims sur des termes qui visent à englober la définition des personnes. L’extension LGBT, ne découle-t-il pas de l’extension de ces catégories et affirmation identité propre. Voit qu’apparaissent au fur et à mesure avec disparition catégories au profit catégories plus individuelles.

## Discussion

Statique décisionnelle

Plus de diversité ? Pour MVR non. Encore plus exacerbé. Identification d’une rupture avec ou sans échelle. Un geste interprétatif chargé de conséquence et pas ontologiquement neutre. Pourrait argumenter de manière contraire qu’il y a une continuité entre les deux. En fait discontinuité dépend de paradigme cognitifs que l’on va prendre. Dépend sur la sélection de paramètres à deux échelles. Dès lors que l’on commence à penser dans un espace vectoriel très différent. Bien sûr toujours de l’échantillonnage. Mais plus sur la représentation bi-directionnel mais espace vectoriel multidimensionnel. On commence à échantillonner sur plusieurs dimensions.

Alors patternes de la gaussiennes, réparaissent. Kamin, dimensions latentes, etc. Ce qui se passe dans les modèles de langues comme word2vec, etc. Ne lisent pas tous les livres possibles, mais les news, et créent des vecteurs à x dimensions pour interpréter.

Idée de chien, des apprentissages qui reconstruisent des représentations statistiques mais multidimensionnels. Pas visibles. Comme la gaussienne est un patern visuel qu’un certain dispositif technologique qui permet ensuite de produire des catégories.

De la même manière distribution latente des mots, profilage, etc. En effet pas d’idéal type, mais en réalité construisent des profils ou encore plus fortement normalisés. Simplement identifie un nombre plus élevé de normalités et ensuite les forcer.

R des cases autant que de personnes. Ne sait pas donc si peut les appeler cases. Mais pas certain qu’un caractère normatif qui découle de ça.
En musique, n’observe pas renforcement comportement normalisés. Nombre de disques d’or ou diamant en chutte libre, car effet masse des médias diffuseur moindre. Toujours des succès mais étant donné l’étalement moins qu’avant.

Lorsque l’on a une loi de puissance comme cela, pas parce qu’il y a une très large distribution que cette répartition n’est pas homogène. Il y a des vidéos sur YouTube qui sont énormément regardées. Mais pas parceque plus regardées qu’a à faire à un phénomène de normalisation. Car toutes les autres touches plus de monde.

Pas sûr que la multidimensionnalité change beaucoup de choses. Sauf lorsque l’on produit les projections, on peut avoir des phénomènes de création de gaussienne, mais néanmoins... Ex des modèles de langues, quand un mot est polysémique, s’aperçoit que la distribution des sens s’observent sur une loi de puissance. Les modèles de langues qui proposent des embeding en contexte sont capables d’identifier cette polysémie à travers cette loi de puissance. Donc pourquoi dire que sont plus gaussiens ?

MVR identifier des paterns distributionnels dans un espace vectoriel.

Caractéristiques des réseau également être sans échelle.
Rétroactions, moins évidentes et plus spéculatives. Mais le fait que soit dans une période d’individualisation, paraît relativement clair.

Individualisme, une normalisation d’un rapport entre la companie et le client, l'humain et la technologie.
Votre argument serait qu’on passe d’un système où peu de catégorie à un système où il y en aurait plus mais la même chose.

Acte herméneutique intéressant distinction sans échelle / avec échelle.

Avec ensemble de vecteurs, identifie un pattern qui permet de dire que telle ou telle ville.

Phénomènes sans échelle, souvent témoins de structures fractales. Fait que ces systèmes soient fractals, ce qui permet d’aller chercher de l’eau qui est très dispersée et non pas concentrée. Pense qu’en effet les systèmes qui vont traiter des informations sans échelle, où l’information est partout de manière diffuse.

Est-ce que augmenter le nombre de catégories abaissent le pouvoir normalisant ou pas. Serait tenter de penser que si 100 catégories injonction moins forte. MRV dit que si profilage si fin que moi, caractère auto-réalisant.

La réalité du monde qui se distribue entre phénomènes avec échelle ou sans échelle. Mais ne pense pas qu’il en existe d’autres. Opposition entre phénomènes multiplicatifs et additifs. Modèle d’Albert et Barabasi modèle d’attachement préférentiel. 

Les distributions utilisées sont des constructions. Ce sont des technologies et en créant ces catégories ont génère des normes.

Modéliser la distribution de la taille des gens, bien sûr une construction. Mais la distribution autour d’une moyenne un état du monde. Fabriquer les outils mathématiques qui rendent compte du monde sont construits mais des théorèmes qui expliquent pourquoi on tend vers ces distributions.

Mais le choix des paramètres est toujours une construction. Car choix de ce qui est significatif ou pas. Algorithmes IA qui choisissent les paramètres de manière différente et projette des catégories. Côté constructif très présent dans la façon dont on va utiliser ces choses là.

Peut-on dire que dans nos comportements on balance entre deux tendances, comme se conformer aux gens auxquels peut s’identifier, et tendance de différentier où essaye de se différentier des groupes. Identité qu’essayerait de gérer soit en se conformant à des attentes soit en s’y opposant.

Quand on a peu de catégories peu de possibilités d’opposition. Systèmes de catégories croisées. Plus grand nombre de catégories normatives, plus de possibilités de se distinguer ou de s’opposer aux normes en vigueur.

Comme linguiste, plutôt tendance à me dire que les disciplines qui me sont proposées ne sont pas satisfaisantes et envie de me construire ailleurs que dans ce système de catégorie. Un peu l’histoire des DHs. Après dans cette diversité peut dire que ne se reconnaît pas. Possibilité de préciser ou revendiquer son identité sans s’appuyer sur ce système de catégorie quelque chose de nouveau qui va dans le sens de l’individualisme qui fait que se revendique en tant que soi plutôt que tel ou tel groupe social. Rendu possible car les catégories moins globalisantes.

Un niveau de spécialisation plus grand, mène à plein de spécialisations. Est-ce que notre vision n’en devient pas biaisée.

Les modèles bayésiens n’ont pas de pré-requis de normalité. 

Les succès des systèmes d’IA rendus possibles car dispose aujourd’hui d’une masse de données conséquente. Et si en a besoin car une masse de données conséquente. Bien parce qu’il y a des phénomènes rares que l’on en a besoin pour couvrir des phénomènes sans échelle qui sont partout.

Même pour la vision. Distinguer un chat d’un lapin. Souvent vont apparaître dans des positions bien distinguées mais aussi une diversité des formes. Lorsque va regarder la fréquence de distribution de ces ofrmes là. Les positions standard bien là, mais des positions plus rares. Là encore métaphore des racines de tout à l’heure, l’information est peu dense, donc besoin de système qui vont la réticuler.

Les systèmes symboliques à base de règles traitent des cas identifiés à l’avance. Finalement ce sont des systèmes catégoriques qui sont très près des lois normales. Mais cela ne marche pas car permet de traiter seulement les cas principaux.
Les systèmes connexionnistes arrivent précisément car ils sont capables d’échantillonner ces grandes masses de données car les phénomènes sont sans échelle et ils forment des catégorisation.

Quand on prend du texte et que l’on fait des embeding, prolongements de mots. Les systèmes contextuels là où sont super, c’est que quand voit le chat mange la souris, etc. chaque mot dans son contexte a sa propre représentation. Et système qui ne va pas prendre chaque catégorie mais telle quelle.

GLOVE pas contextuel, car il fait une représentation par mot. 

Fractal peut permettre d’identifier à différentes échelles.
Mais pas d’émergeance dans la fractale. 

Distinguer fractale mathématiques et quelque chose qui a présente les caractérisques des fractals. Les objets dans la nature ne sont pas exactement de fractales mathématiques.

Exemple du Golfe. Est-ce que les objets qui ne peuvent pas exister dans nos représentations existent vraiment. Exemple physique quantique, outils qui nous permettent de voir la matière ne sont pas neutres. Ces objets existent-ils vraiment ? mais si seulement du côté de nos représentations, alors doit les travailler à notre échelle.

Les deux, il existe. Mais dès lors que le nomme et le représente sur une carte, on lui attribue toute une série de caractéristiques complémentaires qui vont lui donner tout un ensemble de caractéristiques. 

Un gain ou une perte ?
La question ne se pose sans doute pas en ces termes.
En tous les cas une question ontologique fondamentale Karen Baren *Meeting the universe halfway* https://www.dukeupress.edu/meeting-the-universe-halfway
Selon elle, pas d’essence mais une réalité (elle est réaliste), mais la réalité émerge dans des conditions particulières d’observation.




Questions
Quid rapport big data
Cybernétique

En quoi ML et DL pas des statistiques ? création de catégories par rapport loi normales, production d’achétypes
Comment documenter la boucle de rétroaction sociale

En t’écoutant parler, je ne pouvait m’empêcher de penser à certaines applications de l’intelligence artificielle, ou à plus proprement parler du Machine learning. Ces dispositifs ont tendance à refléter les catégories de représentation, d’une certaine manière de produire des idéal type.

Quel statut ont-ils dans le cadre d’analyse que tu proposes ? Technologies fondées sur des catégories statistiques, mais possibilité de prendre en compte grands ensembles sans échelle. Sont-elles réellement des technologies sans échelle ? Comment pourrait s’y adapter ? 

Cependant approches connexionnistes distinctives, cf. Big data. Est-ce qu’un nouveau régime ?


Quid fractal